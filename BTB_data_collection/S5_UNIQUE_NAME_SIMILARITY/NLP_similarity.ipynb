{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Needed libraries:\n",
    "#Regex for text cleaning\n",
    "import re\n",
    "\n",
    "#NLP library\n",
    "import nltk\n",
    "\n",
    "#Helper for creating regex \n",
    "import string\n",
    "\n",
    "# Lemmatisation is the algorithmic process of determining the lemma of a word based on its intended meaning.\n",
    "# Lemmatisation depends on correctly identifying the intended part of speech and meaning\n",
    "#of a word in a sentence, as well as within the larger context surrounding that sentence\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl=WordNetLemmatizer()\n",
    "\n",
    "#pattern.en module contains a fast part-of-speech tagger for English (CLiPS)\n",
    "from pattern.en import tag\n",
    "\n",
    "#WordNet is a lexical database for the English language.[1] It groups English words into sets of synonyms called synsets,#\n",
    "#provides short definitions and usage examples, and records a number of relations among these synonym sets or their members. \n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To delete stop words from the text\n",
    "from nltk.corpus import stopwords\n",
    "stopword_list=stopwords.words(\"english\")\n",
    "\n",
    "#Add additional stop words\n",
    "stopword_list.extend(['www','mail','edu','athttps'])\n",
    "\n",
    "#For tokenizing\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "#remove special characters, this is recommended\n",
    "remove_characters=re.compile('[^a-zA-Z ]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    text = text.decode('utf-8')\n",
    "    text=text.strip()\n",
    "    filtered_sentence=re.sub(remove_characters, r' ', text)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "#pos_tagged_text is lower case and has WordNet tags, ready to lemmatize    \n",
    "    pos_tagged_text = pos_tag_text(text)\n",
    "    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag\n",
    "                         else word #if word has a tag lemmatize it and add to the list, otherwise just add the word                    \n",
    "                         for word, pos_tag in pos_tagged_text]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Annotate text tokens with POS tags\n",
    "def pos_tag_text(text):\n",
    "#Converts Penn Treebank POS tags to WordNet tags    \n",
    "    def penn_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None\n",
    "    #Use pattern library tagging functions (Penn Treebank syntax)\n",
    "    tagged_text = tag(text)# Result: list of tuples for each sentence\n",
    "    #In order to use lemmatizer we need to change POS tags to WordNet tags and make all words lowercase\n",
    "    tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\n",
    "                         for word, pos_tag in\n",
    "                         tagged_text]\n",
    "    return tagged_lower_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function removes stopwords\n",
    "def remove_stopwords(text):\n",
    "    tokens=tokenize_text(text)\n",
    "    filtered_tokens=[token for token in tokens if token not in stopword_list]\n",
    "    filtered_text=\" \".join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This fucntion tokenize words in a sentence\n",
    "def tokenize_text(text):\n",
    "    text = text.decode('utf-8')\n",
    "    tokens=nltk.word_tokenize(text)\n",
    "    tokens=[token.strip() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_abstract(abstracts):\n",
    "    normalized_abstracts=[]\n",
    "    for abstract in abstracts:\n",
    "        normalized_abstract=[]\n",
    "        #First clean data from any special characters\n",
    "        text=remove_special_characters(abstract)\n",
    "        #Split abstract into sentences\n",
    "        sentences=sent_tokenize(text)\n",
    "        for text in sentences:\n",
    "            text=lemmatize_text(text)\n",
    "            text=remove_stopwords(text)\n",
    "            normalized_abstract.append(text)\n",
    "        normalized_abstract_string=\" \".join(normalized_abstract)\n",
    "        normalized_abstracts.append(normalized_abstract_string)\n",
    "    return normalized_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GETTING THE FEACURES AND VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_feature_matrix(abstracts, feature_type='frequency',\n",
    "                         ngram_range=(1, 1), min_df=0.00, max_df=1.0):\n",
    "\n",
    "    feature_type = feature_type.lower().strip()  \n",
    "    \n",
    "    if feature_type == 'frequency':\n",
    "        vectorizer = CountVectorizer(binary=False, min_df=min_df,\n",
    "                                     max_df=max_df, ngram_range=ngram_range)\n",
    "    elif feature_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, \n",
    "                                     ngram_range=ngram_range,use_idf=True)\n",
    "    else:\n",
    "        raise Exception(\"Wrong feature type entered. Possible values:'frequency', 'tfidf'\")\n",
    "\n",
    "    feature_matrix = vectorizer.fit_transform(abstracts).astype(float)\n",
    "\n",
    "    \n",
    "    return vectorizer, feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read im the data, delete additional columns\n",
    "\n",
    "all_articles_tools = pd.read_table('ALL_journals_NEW.txt', keep_default_na=False)\n",
    "#Get rid of unnnamed columns added each time you read data into df\n",
    "all_articles_tools.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "all_articles_tools.drop('Unnamed: 0.1', axis=1, inplace=True)\n",
    "all_articles_tools.drop('Unnamed: 0.1.1', axis=1, inplace=True)\n",
    "#all_articles_tools.drop('Unnamed: 0.1.1.1', axis=1, inplace=True)\n",
    "# create a column for storing related articles\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7382"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_articles_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#this column is going to be used to store names of tools without any integers inside\n",
    "# this is helpful to find different versions of the same tools\n",
    "#I am stripping names of special character and numbers to easier identify articles related to the same tool\n",
    "all_articles_tools[\"main_name\"]=\"NULL\"\n",
    "\n",
    "first_word=re.compile('[^\\s]+')\n",
    "for i in range(len(all_articles_tools)):\n",
    "    try:\n",
    "        tool_name=all_articles_tools[\"name_tool\"][i]\n",
    "        tool_name=re.match(first_word, tool_name).group(0)\n",
    "        tool_name = ''.join([j for j in tool_name if not j.isdigit()])\n",
    "        all_articles_tools[\"main_name\"][i]=tool_name\n",
    "    except:\n",
    "        pass\n",
    "    i=i+1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_articles_tools[\"related_articles\"]=\"NULL\"\n",
    "\n",
    "#Give each data point an unique id\n",
    "id_list=range(len(all_articles_tools)+1)[1:]\n",
    "all_articles_tools[\"id\"] = id_list\n",
    "\n",
    "names_of_tools=all_articles_tools.main_name.tolist()\n",
    "# create a list of names of tools that are duplicates\n",
    "import collections\n",
    "duplicated_tools=[item for item, count in collections.Counter(names_of_tools).items() if count > 1]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#list of id of articles which are not \"main articles associated with a tool\"- to be dropped before calculating\n",
    "#cosine similarity\n",
    "articles_to_drop_by_ids=[]\n",
    "\n",
    "for name in duplicated_tools:\n",
    "#ascending = False most recent on the top of the df- we decided that the most recent article\n",
    "# will be listed as the main on the tools' landing page\n",
    "    related_articles=all_articles_tools.loc[all_articles_tools['main_name'] == name].sort_values(by='date', ascending=False)\n",
    "    ids=[]\n",
    "    ids=related_articles.id.tolist()\n",
    "    index=ids[0]-1\n",
    "    all_articles_tools[\"related_articles\"][index]=ids[1:]\n",
    "    articles_to_drop_by_ids.append(ids[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Another piece of information needed for calculating relative citations, per year citations\n",
    "year_pattern=re.compile('^20[0-9]{2}')\n",
    "\n",
    "all_articles_tools[\"year\"]=\"NULL\"\n",
    "for i in range(len(all_articles_tools)):\n",
    "    try:\n",
    "        year_matched=re.match( year_pattern, all_articles_tools[\"date\"][i] ).group(0)\n",
    "        all_articles_tools[\"year\"][i] = year_matched\n",
    "\n",
    "    except:\n",
    "        all_articles_tools[\"year\"][i]=\"NULL\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MAKE SURE VIEWS, CITATIONS AND ALTMETRIC SCORE ARE NP INTS -this was giving problems during calculations\n",
    "# of citations per year and so on\n",
    "all_articles_tools.altmetric_score = all_articles_tools.altmetric_score.astype(np.int64)\n",
    "all_articles_tools.views = all_articles_tools.views.astype(np.int64)\n",
    "all_articles_tools.citations_amount = all_articles_tools.citations_amount.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "all_articles_tools[\"citations_per_year\"]=0.0\n",
    "#Set relative amount of citations- citations per year\n",
    "for i in range(len(all_articles_tools)):\n",
    "    if all_articles_tools[\"citations_amount\"][i] is None:\n",
    "        pass\n",
    "    else:\n",
    "        years=2017 - int(all_articles_tools[\"year\"][i]) +1\n",
    "        yearly_citations=float(all_articles_tools[\"citations_amount\"][i])/float(years) \n",
    "        all_articles_tools[\"citations_per_year\"][i]=int(round(yearly_citations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "all_articles_tools[\"views_per_year\"]=0\n",
    "for i in range(len(all_articles_tools)):\n",
    "    years=2017 - int(all_articles_tools[\"year\"][i])+1\n",
    "    yearly_views= float(all_articles_tools[\"views\"][i]/years)\n",
    "    all_articles_tools[\"views_per_year\"][i]=yearly_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "#SET RELATIVE AMOUNT OF VIEWS AND SCALE (relative views are in range 4 to 34 so they can be use as sizes of balls\n",
    "#on scatter plots)\n",
    "#The 4 to 34 scale was introduce because it translates nicely to a size of a ball in the Plotly 3D scatterplot\n",
    "#This can be changed or dropped\n",
    "all_articles_tools[\"relative_views\"]=0\n",
    "max_views=all_articles_tools[\"views_per_year\"].max()\n",
    "\n",
    "\n",
    "length=len(all_articles_tools)       \n",
    "for i in range(length):\n",
    "    yearly_views=float(all_articles_tools[\"views_per_year\"][i])\n",
    "    relative_views=(yearly_views/float(max_views))*30+4\n",
    "    all_articles_tools[\"relative_views\"][i]=int(round(relative_views))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#change a list of lists into a regular list\n",
    "articles_to_drop_by_ids = [item for sublist in articles_to_drop_by_ids for item in sublist]\n",
    "\n",
    "# list of ids of main articles\n",
    "id_main_articles=range(len(all_articles_tools)+1)[1:]\n",
    "for item in articles_to_drop_by_ids:\n",
    "    id_main_articles.remove(item)\n",
    "    \n",
    "#Create a df only with TOOLS- main articles \n",
    "only_tools=DataFrame()\n",
    "only_tools=all_articles_tools[[\"homepage\", \"id\", \"related_articles\", \"name_tool\", \"abstract\", \"citations_per_year\",\"relative_views\", \"topics\" ]]\n",
    "\n",
    "# drop the values which are not main articles\n",
    "for item in articles_to_drop_by_ids:\n",
    "    only_tools = only_tools[only_tools.id != item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_tools = only_tools.rename(index=str, columns={\"id\": \"id_article\", \"related_articles\": \"ids_related_articles\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At this point I have two dataframes:\n",
    "* one with MAIN TOOLS - each of them will have their own landing page on the website,\n",
    "* one with all articles - I keep them for related articles, they also help with foregin keys and tables organization in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Give each data point an unique id\n",
    "id_list=range(len(only_tools)+1)[1:]\n",
    "only_tools[\"id\"] = id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_abstracts=only_tools.abstract.tolist()\n",
    "names_of_tools=only_tools.name_tool.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# At this point I need to work more on my abstracts and names_of tools to get better results:\n",
    "# These two things show to improve the results: \n",
    "# -add normalized names of tools to stop words,\n",
    "# -delete email addresses from the abstracts\n",
    "# Getting rid of theses info also help to make keywords more relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalizing names of tools so they have the same form as names of tools in normalized abstracs\n",
    "names_copy=only_tools.name_tool.tolist()\n",
    "all_normalized_names=normalize_abstract(names_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add normalized tool's names to the stopwords, they are not relavant for similarity calculation,faster calculation\n",
    "stopword_list.extend(['www','mail','edu','athttps'])\n",
    "stopword_list.extend(all_normalized_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18169"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##REGEX FOR GETTING RID OF EMIALS AND WEBSITE ADDRESSES\n",
    "website_pattern=re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "email_pattern=re.compile(r'http[^\\s]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "for i in range(len(all_abstracts)):\n",
    "    result=re.sub(website_pattern,\"\",  all_abstracts[i])\n",
    "    all_abstracts[i]=re.sub(email_pattern,\"\",  result)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### This part deals with similarity between abstracts' of MAIN TOOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: NORMALIZE YOUR DATA\n",
    "all_normalized_abstracts=normalize_abstract(all_abstracts)\n",
    "\n",
    "# Step 2: EXTRACT FEATURES\n",
    "tfidf_vectorizer, tfidf_matrix=build_feature_matrix(all_normalized_abstracts, feature_type=\"tfidf\")\n",
    "\n",
    "#Get the names of the features in the features matrix, so you are aware of what is happening\n",
    "feature_names=tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "#Calculate the adjacency matrix\n",
    "adj_matrix=cosine_similarity(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Here we save 9 closest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This would add 9 closets neighbors index and similarity level for recommandaction system purposes\n",
    "only_tools[\"closets_neighbor_1\"]=\"NULL\"\n",
    "only_tools[\"closets_neighbor_2\"]=\"NULL\"\n",
    "only_tools[\"closets_neighbor_3\"]=\"NULL\"\n",
    "only_tools[\"closets_neighbor_4\"]=\"NULL\"\n",
    "only_tools[\"closets_neighbor_5\"]=\"NULL\"\n",
    "only_tools[\"closets_neighbor_6\"]=\"NULL\"\n",
    "only_tools[\"closets_neighbor_7\"]=\"NULL\"\n",
    "only_tools[\"closets_neighbor_8\"]=\"NULL\"\n",
    "only_tools[\"closets_neighbor_9\"]=\"NULL\"\n",
    "only_tools[\"similarity_neighbor_1\"]=\"NULL\"\n",
    "only_tools[\"similarity_neighbor_2\"]=\"NULL\"\n",
    "only_tools[\"similarity_neighbor_3\"]=\"NULL\"\n",
    "only_tools[\"similarity_neighbor_4\"]=\"NULL\"\n",
    "only_tools[\"similarity_neighbor_5\"]=\"NULL\"\n",
    "only_tools[\"similarity_neighbor_6\"]=\"NULL\"\n",
    "only_tools[\"similarity_neighbor_7\"]=\"NULL\"\n",
    "only_tools[\"similarity_neighbor_8\"]=\"NULL\"\n",
    "only_tools[\"similarity_neighbor_9\"]=\"NULL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "data_length=len(only_tools)\n",
    "for i in range (data_length):\n",
    "    sorted_similarity=sorted(((value, index) for index, value in enumerate(adj_matrix[i])), reverse=True)\n",
    "    closest_list=[]\n",
    "    closest_list=sorted_similarity[1:14]\n",
    "#set up the values of the closest neighboors\n",
    "    only_tools[\"closets_neighbor_1\"][i] = (closest_list[0][1] + 1)# index to id\n",
    "    only_tools[\"similarity_neighbor_1\"][i] = closest_list[0][0]\n",
    "#    \n",
    "    only_tools[\"closets_neighbor_2\"][i] =(closest_list[1][1] + 1)\n",
    "    only_tools[\"similarity_neighbor_2\"][i] = closest_list[1][0]\n",
    "#    \n",
    "    only_tools[\"closets_neighbor_3\"][i] =(closest_list[2][1] + 1)\n",
    "    only_tools[\"similarity_neighbor_3\"][i] = closest_list[2][0]\n",
    "#    \n",
    "    only_tools[\"closets_neighbor_4\"][i] =(closest_list[3][1] + 1)\n",
    "    only_tools[\"similarity_neighbor_4\"][i] = closest_list[3][0]\n",
    "    \n",
    "    only_tools[\"closets_neighbor_5\"][i] = (closest_list[4][1] + 1)# index to id\n",
    "    only_tools[\"similarity_neighbor_5\"][i] = closest_list[4][0]\n",
    "#    \n",
    "    only_tools[\"closets_neighbor_5\"][i] =(closest_list[5][1] + 1)\n",
    "    only_tools[\"similarity_neighbor_5\"][i] = closest_list[5][0]\n",
    "#    \n",
    "    only_tools[\"closets_neighbor_6\"][i] =(closest_list[6][1] + 1)\n",
    "    only_tools[\"similarity_neighbor_6\"][i] = closest_list[6][0]\n",
    "#    \n",
    "    only_tools[\"closets_neighbor_7\"][i] =(closest_list[7][1] + 1)\n",
    "    only_tools[\"similarity_neighbor_7\"][i] = closest_list[7][0]\n",
    "    \n",
    "    only_tools[\"closets_neighbor_8\"][i] =(closest_list[8][1] + 1)\n",
    "    only_tools[\"similarity_neighbor_8\"][i] = closest_list[8][0]\n",
    "    \n",
    "    only_tools[\"closets_neighbor_9\"][i] =(closest_list[9][1] + 1)\n",
    "    only_tools[\"similarity_neighbor_9\"][i] = closest_list[9][0]\n",
    "\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## It would be alaso beneficial to store normalized abstracts for all of the articles for more efficient search in case other method don not give enough results or any results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_abstracts_stopword_list=stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_abstracts_ALL_articles=all_articles_tools.abstract.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##REGEX FOR GETTING RID OF EMIALS AND WEBSITE ADDRESSES\n",
    "website_pattern=re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "email_pattern=re.compile(r'http[^\\s]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "for i in range(len(all_abstracts_ALL_articles)):\n",
    "    result=re.sub(website_pattern,\"\",  all_abstracts[i])\n",
    "    all_abstracts[i]=re.sub(email_pattern,\"\",  result)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: NORMALIZE YOUR DATA\n",
    "all_normalized_abstracts_ALL_articles=normalize_abstract(all_abstracts_ALL_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_articles_tools[\"abstract_normalized\"]=\"NULL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_articles_tools)):\n",
    "    all_articles_tools[\"abstract_normalized\"][i]=all_normalized_abstracts_ALL_articles[i]\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, it is time to get relevant keywords for the landing pages of tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17746"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# That many features are avaiable\n",
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5995, 17746)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#matrix shape is number of abstracts (rows) by number of unique words (columns)\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BUILD LIST you don't care about in your corpous to add to stopwords\n",
    "# If there are meaningless words in feature_names list- they should be added to stop words and adj_matrix should \n",
    "# be them recalculated OR just delete them from the accepted keywords list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract the most important features ~ keywords that define the tool, abstract\n",
    "# In order to identify words which were given most weight by tf-idf we need to extract wieights from tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In order to analyze rows of the matrix we need to change it to dense\n",
    "from scipy.sparse import csr_matrix\n",
    "matrix_dense=tfidf_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_dense #This is a very sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add columns to store info about most importat features=words, and their assosiated weights \n",
    "only_tools[\"feature_list\"]=\"NULL\"\n",
    "only_tools[\"feature_scores\"]=\"NULL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I chose weight of word to be more than 0.10 for word to be considered meaningful\n",
    "#Here we accually collect only index of these words in the list of all the features\n",
    "#In the next step we will change index to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The genes that produce antibodies and the immune receptors expressed on lymphocytes are not germline encoded; rather, they are somatically generated in each developing lymphocyte by a process called V(D)J recombination, which assembles specific, independent gene segments into mature composite genes. The full set of composite genes in an individual at a single point in time is referred to as the immune repertoire. V(D)J recombination is the distinguishing feature of adaptive immunity and enables effective immune responses against an essentially infinite array of antigens. Characterization of immune repertoires is critical in both basic research and clinical contexts. Recent technological advances in repertoire profiling via high-throughput sequencing have resulted in an explosion of research activity in the field. This has been accompanied by a proliferation of software tools for analysis of repertoire sequencing data. Despite the widespread use of immune repertoire profiling and analysis software, there is currently no standardized format for output files from V(D)J analysis. Researchers utilize software such as IgBLAST and IMGT/High V-QUEST to perform V(D)J analysis and infer the structure of germline rearrangements. However, each of these software tools produces results in a different file format, and can annotate the same result using different labels. These differences make it challenging for users to perform additional downstream analyses. To help address this problem, we propose a standardized file format for representing V(D)J analysis results. The proposed format, VDJML, provides a common standardized format for different V(D)J analysis applications to facilitate downstream processing of the results in an application-agnostic manner. The VDJML file format specification is accompanied by a support library, written in C++ and Python, for reading and writing the VDJML file format. The VDJML suite will allow users to streamline their V(D)J analysis and facilitate the sharing of scientific knowledge within the community. The VDJML suite and documentation are available from https://vdjserver.org/vdjml/ . We welcome participation from the community in developing the file format standard, as well as code contributions.'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_tools[\"abstract\"][2558]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for i in range(len(only_tools)):\n",
    "    matrix_dense_row=matrix_dense[i]\n",
    "    A = np.squeeze(np.asarray(matrix_dense_row))\n",
    "    important_features=[(d, x) for d, x in enumerate(A) if x > 0.10]\n",
    "    feature_tuples=zip(*important_features)\n",
    "    feature_lists=map(list,feature_tuples )\n",
    "    only_tools[\"feature_list\"][i]=feature_lists[0]\n",
    "    only_tools[\"feature_scores\"][i]=feature_lists[1]\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This column will store actual words\n",
    "only_tools[\"feature_names\"]=\"NULL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# feature indexes to words\n",
    "for i in range(len(only_tools)):\n",
    "    tool_features=[]\n",
    "    for item in only_tools[\"feature_list\"][i]:\n",
    "        tool_features.append(feature_names[item])\n",
    "    only_tools[\"feature_names\"][i]=tool_features\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add all the list of feature words into one big list check frequency of accurence of the words\n",
    "# choose only globally relevant words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add all the words you selected for each of the abstracts into one big list\n",
    "big_list_of_words=[]\n",
    "for i in range(len(only_tools)):\n",
    "    big_list_of_words.append(only_tools[\"feature_names\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of lists into a list- this is very long list with the same words appearing multiple times\n",
    "features_list = [item for sublist in big_list_of_words for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add to feature list keywords from the journal itself\n",
    "#They are stored as one string - words comma separated -change the string into list of words\n",
    "only_tools[\"topics_list\"]=\"NULL\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "big_list_of_topics=[]\n",
    "for i in range(len(only_tools)):\n",
    "    topics=\"\"\n",
    "    topics= only_tools[\"topics\"][i]\n",
    "    topics1=topics.split(\",\")\n",
    "    only_tools[\"topics_list\"][i]=topics1\n",
    "    big_list_of_topics.append(only_tools[\"topics_list\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of lists into a list\n",
    "topics_list = [item for sublist in big_list_of_topics for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature list includes keywords with weigth more than 0.1\n",
    "#and keywords as provided by the journals-only NAR and BMC provide the journals)\n",
    "features_topics_list=features_list + topics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Some of the features have leading space-we want to get rid of it\n",
    "leading_space=re.compile(\"^\\s\")\n",
    "for i in range(len(features_topics_list)):\n",
    "    word=re.sub(leading_space, \"\",features_topics_list[i])\n",
    "    features_topics_list[i]=word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This library helps to identify \"important words\"-as per weight that are common enough to be good tahs -keywords\n",
    "import collections\n",
    "counter=collections.Counter(features_topics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To get all words=features with weight at least 0.10 is not enough, we want to only consider words that\n",
    "#were popular and meaningful. #We can manulally decided how many distinct keywords we want to include \n",
    "#and remove words which have high weight but are not meaningful for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_keywords_tuples=counter.most_common(319) # 300+ keywords proved to work well but can be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_keywords = zip(*my_keywords_tuples)\n",
    "my_keywords =my_keywords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#List of words that are in keywords list but are not meaningfull -continue to develop it\n",
    "black_list_of_terms=[\"\", \"co\", \",many\", \"non\",\"end\", \"may\", \"tf\",\"know\", \"use\",\"via\",\"onto\",\"five\", \"four\",\n",
    "                     \"yet\", \"without\",\"go\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_keywords_clean = [x for x in my_keywords if x not in black_list_of_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Add all the considered keywords for each abstract into one list\n",
    "only_tools[\"keywords_dirty\"]=\"NULL\"\n",
    "for i in range(len(only_tools)):\n",
    "    only_tools[\"keywords_dirty\"][i]= only_tools[\"feature_names\"][i] + only_tools[\"topics_list\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#That many different keywords which willl be featured on the landing pages\n",
    "len(my_keywords_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This column will store these relevant keywords for each of the tools\n",
    "only_tools[\"tool_keywords\"]=\"NULL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:4: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#Saving only relevant keywords\n",
    "for i in range(len(only_tools)):\n",
    "    all_features=only_tools[\"keywords_dirty\"][i]\n",
    "    only_relevant = [word for word in all_features if word  in my_keywords_clean]\n",
    "    only_tools[\"tool_keywords\"][i]=only_relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### *At this point the relevant keywords for each tool are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This part deals with improving search results using CHAMPIONSHIP LIST, as described in https://nlp.stanford.edu/IR-book/ with slight modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#List of words that are in keywords list but are not meaningfull -continue to develop it\n",
    "#This could be improved but the words look pretty relevant\n",
    "black_list_of_terms_search=[\"\", \"co\", \",many\", \"non\",\"end\", \"may\", \"tf\",\"know\", \"use\",\"via\",\"onto\",\"five\", \"four\",\n",
    "                     \"yet\", \"without\",\"go\", \"june\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#top 4,000 words for search\n",
    "search_words_tuples=counter.most_common(4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_search_keywords = zip(*search_words_tuples)\n",
    "my_search_keywords =my_search_keywords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_search_words_clean = [x for x in my_search_keywords if x not in black_list_of_terms_search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save the words which can be used to find a given tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/pat/venv/lib/python2.7/site-packages/ipykernel_launcher.py:5: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "only_tools[\"search_words\"]=\"NULL\"\n",
    "#Saving only relevant keywords\n",
    "for i in range(len(only_tools)):\n",
    "    all_features=only_tools[\"keywords_dirty\"][i]\n",
    "    only_relevant = [word for word in all_features if word in my_search_words_clean]\n",
    "    only_tools[\"search_words\"][i]=only_relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the results: abstract and corresponding search words that will 'find' it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'summary software program measure selective influence structural biochemical amino acid property cladogenesis perform goodness categorical statistical test availability package executables window pc macintosh osx java code documentation instruction manual available unix version available upon request contact correspondence address'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_normalized_abstracts[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'acid',\n",
       " u'address',\n",
       " u'amino',\n",
       " u'biochemical',\n",
       " u'categorical',\n",
       " u'code',\n",
       " u'correspondence',\n",
       " u'documentation',\n",
       " u'executables',\n",
       " u'goodness',\n",
       " u'influence',\n",
       " u'instruction',\n",
       " u'java',\n",
       " u'macintosh',\n",
       " u'manual',\n",
       " u'measure',\n",
       " u'osx',\n",
       " u'package',\n",
       " u'pc',\n",
       " u'perform',\n",
       " u'program',\n",
       " u'property',\n",
       " u'request',\n",
       " u'selective',\n",
       " u'statistical',\n",
       " u'structural',\n",
       " u'test',\n",
       " u'unix',\n",
       " u'upon',\n",
       " u'version',\n",
       " u'window']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_tools[\"search_words\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create new data structure to hold the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This help with fixing tool's indexing\n",
    "search_tools=pd.concat([only_tools],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d={'search_word': my_search_words_clean}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_words_df=DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Search words df is a CHAMPIONSHIP LIST, it stores a words and list of tools' ids that are findiable by \n",
    "# that search keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_words_df[\"tool_ids\"]=np.empty((len(search_words_df), 0)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#search_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#take search words associated with a tool and put tools' id next to that words in the df\n",
    "for i in range(len(only_tools)):\n",
    "    all_search_words=only_tools[\"search_words\"][i]\n",
    "    for word in all_search_words:\n",
    "#        print(word)\n",
    "        index_list=search_words_df.loc[search_words_df['search_word'] == word].index.tolist()\n",
    "        search_words_df[\"tool_ids\"][index_list[0]].append(only_tools[\"id\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#search_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The best way to work with this would be have words in one list and then list of lists for tools_id\n",
    "#This could be improved by making the words in alphabetic order\n",
    "search_df=search_words_df.sort_values( [\"search_word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#search_df[\"search_word\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_words_list=search_df[\"search_word\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thefile = open('search_words_t.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thefile.write(\"[\")\n",
    "for item in search_words_list:\n",
    "  thefile.write(\"%s,\" % item)\n",
    "thefile.write(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seach_tool_ids=search_df[\"tool_ids\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thefile2 = open('search_tool_t.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thefile2.write(\"[\")\n",
    "for item in seach_tool_ids:\n",
    "  thefile2.write(\"%s,\" % item)\n",
    "thefile2.write(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check the files sometimes the list are cut before- not whole list is written to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#search_df[3959:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fix weird problem \n",
    "my_list=search_df[\"tool_ids\"][3959:].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Matrix also needs to be printed to a file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can build feature vector for each tool based on 4,000 search words we care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: NORMALIZE YOUR DATA\n",
    "all_normalized_abstracts=normalize_abstract(all_abstracts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2: EXTRACT FEATURES\n",
    "tfidf_vectorizer2, tfidf_matrix2=build_feature_matrix_search(all_normalized_abstracts, feature_type=\"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_feature_matrix_search(abstracts, feature_type='frequency',\n",
    "                         ngram_range=(1, 1), min_df=0.00, max_df=1.0,vocabulary=search_words_list):\n",
    "\n",
    "    feature_type = feature_type.lower().strip()  \n",
    "    \n",
    "    if feature_type == 'frequency':\n",
    "        vectorizer = CountVectorizer(binary=False, min_df=min_df,\n",
    "                                     max_df=max_df, ngram_range=ngram_range)\n",
    "    elif feature_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, \n",
    "                                     ngram_range=ngram_range,use_idf=True,vocabulary=search_words_list)\n",
    "    else:\n",
    "        raise Exception(\"Wrong feature type entered. Possible values:'frequency', 'tfidf'\")\n",
    "\n",
    "    feature_matrix = vectorizer.fit_transform(abstracts).astype(float)\n",
    "\n",
    "    \n",
    "    return vectorizer, feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3986"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5995, 3986)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_matrix2=tfidf_matrix2.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_sparse_csr(\"test.txt\", tfidf_matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_sparse_csr(filename,array):\n",
    "    np.savez(filename,data = array.data ,indices=array.indices,\n",
    "             indptr =array.indptr, shape=array.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return csr_matrix((  loader['data'], loader['indices'], loader['indptr']),\n",
    "                         shape = loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydata=load_sparse_csr(\"test.txt.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5995x3986 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 393729 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Here is everything connected to graphing tool in a 3d interactive plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adj_matrix_nor =adj_matrix / adj_matrix.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in order to use t-sne you need to change cosine similarity to cosine distance \n",
    "#cosine distance = 1 - cosine similarity\n",
    "#REST OF THE CODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Before you RUN it take a look at the parameters\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#MODEL 3D\n",
    "model3D=TSNE(n_components=3, perplexity=15.0, early_exaggeration=4.0, learning_rate=100.0, n_iter=8000, n_iter_without_progress=30, min_grad_norm=1e-07, metric='precomputed', init='random', verbose=0, random_state=None, method='barnes_hut', angle=0.5)\n",
    "np.set_printoptions(suppress=True)\n",
    "TSNE_data3D=model3D.fit_transform(1-adj_matrix_nor) \n",
    "transformed_TSNE_data3D=TSNE_data3D.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MODEL 2D\n",
    "\n",
    "model2D=TSNE(n_components=2, perplexity=10.0, early_exaggeration=4.0, learning_rate=100.0, n_iter=8000, n_iter_without_progress=30, min_grad_norm=1e-07, metric='precomputed', init='random', verbose=0, random_state=None, method='barnes_hut', angle=0.5)\n",
    "np.set_printoptions(suppress=True)\n",
    "TSNE_data2D=model2D.fit_transform(1-adj_matrix_nor) \n",
    "transformed_TSNE_data2D=TSNE_data2D.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Manipulating data frame to add new information\n",
    "\n",
    "#addig additional columns to data frame\n",
    "only_tools[\"x\"]=0\n",
    "only_tools[\"y\"]=0\n",
    "only_tools[\"z\"]=0\n",
    "only_tools[\"closest_neighbors\"]=\"NULL\"\n",
    "only_tools[\"x_2d\"]=0\n",
    "only_tools[\"y_2d\"]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#assign the right values to the columns  3D case\n",
    "x_coordinate = transformed_TSNE_data3D[0]\n",
    "len(x_coordinate)\n",
    "only_tools[\"x\"]=x_coordinate\n",
    "\n",
    "y_coordinate = transformed_TSNE_data3D[1]\n",
    "only_tools[\"y\"] = y_coordinate\n",
    "\n",
    "z_coordinate = transformed_TSNE_data3D[2]\n",
    "only_tools[\"z\"]=z_coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#assign the right values to the columns  2D case\n",
    "x_coordinate_2d = transformed_TSNE_data2D[0]\n",
    "only_tools[\"x_2d\"]=x_coordinate_2d\n",
    "\n",
    "y_coordinate_2d = transformed_TSNE_data2D[1]\n",
    "only_tools[\"y_2d\"] = y_coordinate_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print results to files which will be turn into relavant tables in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print vector for each tool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_tools.to_csv(\"backup3_search_main_tools.txt\",sep='\\t', encoding='utf-8')\n",
    "all_articles_tools.to_csv(\"backup3_search_tools_articles.txt\",sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### This is code to get a file that would be input to network2canvas algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['main_tool' 'similar_tool_fk' 'similarity'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-233-8c6cbdea994c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#CODE FOR CANVAS VISUALIZATION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msimilar_tool_canvas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0monly_tools\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"main_tool\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"similar_tool_fk\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"similarity\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/pat/venv/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1956\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1958\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1959\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pat/venv/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2000\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2001\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2002\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2003\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pat/venv/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1229\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s not in index'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobjarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['main_tool' 'similar_tool_fk' 'similarity'] not in index\""
     ]
    }
   ],
   "source": [
    "#CODE FOR CANVAS VISUALIZATION\n",
    "similar_tool_canvas=only_tools[[\"main_tool\", \"similar_tool_fk\", \"similarity\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(similar_tool_canvas)):\n",
    "    tool_id=similar_tool_canvas[\"main_tool\"][i]\n",
    "    similar_tool_canvas[\"main_tool\"][i]= tools[\"name_tool\"][tool_id-1]\n",
    "    similar_tool_id=similar_tool_canvas[\"similar_tool_fk\"][i]\n",
    "    similar_tool_canvas[\"similar_tool_fk\"][i]= tools[\"name_tool\"][similar_tool_id-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similar_tool_canvas.to_csv(\"canvas2.txt\", header=False, index=False,sep='\\t', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
